{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3a3f24-cb6f-4f0a-b21f-58c6e599d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Árbol de Decisión ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.82       119\n",
      "           1       0.80      0.90      0.84       121\n",
      "\n",
      "    accuracy                           0.83       240\n",
      "   macro avg       0.84      0.83      0.83       240\n",
      "weighted avg       0.84      0.83      0.83       240\n",
      "\n",
      "\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       119\n",
      "           1       0.88      0.87      0.88       121\n",
      "\n",
      "    accuracy                           0.88       240\n",
      "   macro avg       0.88      0.88      0.88       240\n",
      "weighted avg       0.88      0.88      0.88       240\n",
      "\n",
      "Columnas detectadas: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "Dimensiones: (1599, 12)\n",
      "\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       413\n",
      "           1       0.55      0.45      0.49        67\n",
      "\n",
      "    accuracy                           0.87       480\n",
      "   macro avg       0.73      0.69      0.71       480\n",
      "weighted avg       0.86      0.87      0.87       480\n",
      "\n",
      "\n",
      "--- Regresión Logística ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       413\n",
      "           1       0.53      0.30      0.38        67\n",
      "\n",
      "    accuracy                           0.86       480\n",
      "   macro avg       0.71      0.63      0.65       480\n",
      "weighted avg       0.84      0.86      0.85       480\n",
      "\n",
      "\n",
      "--- Principales Reglas de Asociación ---\n",
      "  antecedents consequents  support  confidence      lift\n",
      "0     (Carne)     (Arroz)    0.155    0.413333  1.087719\n",
      "1     (Arroz)     (Carne)    0.155    0.407895  1.087719\n",
      "2     (Pasta)     (Arroz)    0.135    0.428571  1.127820\n",
      "3     (Queso)     (Carne)    0.140    0.400000  1.066667\n",
      "4    (Huevos)    (Cereal)    0.140    0.405797  1.248606\n",
      "5    (Cereal)    (Huevos)    0.140    0.430769  1.248606\n",
      "6    (Cereal)     (Pollo)    0.135    0.415385  1.170098\n",
      "7    (Huevos)    (Yogurt)    0.150    0.434783  1.191185\n",
      "8    (Yogurt)    (Huevos)    0.150    0.410959  1.191185\n",
      "9     (Pasta)      (Jugo)    0.135    0.428571  1.113173\n"
     ]
    }
   ],
   "source": [
    "# Nombre Jose Angel Graciano Hernandez\n",
    "# Matricula 100039989\n",
    "# Asignatura Inteligencia Artificial\n",
    "# Fecha 4/10/2025\n",
    "\n",
    "# este proyecto analiza datos usando técnicas o algotimos como KNN, KMeans, PCA entre otas\n",
    "# se construyeron modelos supervisados para clasificar vinos y comparar algoritmos los cuales son arboles de decision, random forest, KNN y regresión logística.\n",
    "# random forest mostro mejor desempeño pero KNN capturo patrones locales en los datos de vinos.\n",
    "# se aplico K-Means para agrupar datos sin etiquetas y se observaron grupos bien definidos.\n",
    "# se generaron reglas de asociacion para ver cuales productos que se compran juntos el cual lo veo util para promociones o marketing.\n",
    "# los resultados muestran que combinar modelos y analisis exploratorio ayuda a tomar mejores decisiones.\n",
    "\n",
    "#Aqui importamos las librerias que vamos a utilizar y tambien al sistema operativo para poder hacer cambios en el \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import os\n",
    "\n",
    "# En caso de que no tengamos ninguna carpeta o solo tengamos este .py creamos las carpetas\n",
    "os.makedirs('practica_ia/figuras', exist_ok=True) \n",
    "os.makedirs('practica_ia/datos', exist_ok=True)\n",
    "#Aqui con ese makedirs hacemos las carpetas y el exist_ok va a ser que si las carpetan esta creada no la haga nuevamente\n",
    "\n",
    "\n",
    "#  parte 1 — arboles de decisión y random forest\n",
    "\n",
    "# aqui clasificaremos los data set sintentico \n",
    "X, y = make_classification(n_samples=800, n_features=6, n_informative=4, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# aqui en el arbol de desicion ponemos que el random state sea de 42 como lo fue pedido en la asignatura\n",
    "clf_tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "print(\"\\n--- Árbol de Decisión ---\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# aqui podremos visualizar el arbol de decision de manera grafica y este sera guardado en la carpeta de figuras con el nombre de arbol_decision\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_tree(clf_tree, filled=True, feature_names=[f\"feat_{i}\" for i in range(X.shape[1])])\n",
    "plt.title(\"Árbol de Decisión\")\n",
    "plt.savefig('practica_ia/figuras/arbol_decision.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# aqui en este aprendizaje supervisado el cual es random forest tendremos el mismo random state que utilzamos antes\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "print(classification_report(y_test, y_pred_rf)) #aqui saldra el reporte de la clasificacion del random forest\n",
    "\n",
    "# Importancia de características\n",
    "importances = pd.Series(rf.feature_importances_, index=[f\"feat_{i}\" for i in range(X.shape[1])])\n",
    "importances.sort_values().plot(kind='barh', title='Importancia de Características (RF)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('practica_ia/figuras/importancia_rf.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# parte 2 — comparación KNN vs regresión logística con dataset de vinos\n",
    "\n",
    "# Carga del dataset de vinos\n",
    "wine = pd.read_csv('practica_ia/datos/calidad_de_vinos.csv', sep=';', quotechar='\"')\n",
    "\n",
    "# aqui lo que hace es limpiar y verificar columnas\n",
    "wine = wine.dropna()\n",
    "print(\"Columnas detectadas:\", wine.columns.tolist())\n",
    "print(\"Dimensiones:\", wine.shape)\n",
    "\n",
    "# y verificar por ultimo si la columna quality existe ya que aqui dice la calidad del vino de no haber dicha columna dira que no la hay\n",
    "if 'quality' not in wine.columns:\n",
    "    raise ValueError('Se requiere columna quality en dataset de vinos')\n",
    "\n",
    "# aqui se crear una variable objetiva binaria la cual  1 = buen vino y 0 = regular\n",
    "y_w = (wine['quality'] >= 7).astype(int)\n",
    "X_w = wine.drop(columns=['quality'])\n",
    "\n",
    "# aqui se realiza el escalado\n",
    "esc = StandardScaler()\n",
    "X_w_scaled = esc.fit_transform(X_w)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_scaled, y_w, test_size=0.3, random_state=42)\n",
    "\n",
    "# aqui inicia le algoritmo el vecino mas cercano o KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"\\n--- KNN ---\")\n",
    "print(classification_report(y_test, y_pred_knn)) #aqui imprime los resultados del KNN\n",
    "\n",
    "# aqui inicia el algoritmo regresion logistica\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Regresión Logística ---\")\n",
    "print(classification_report(y_test, y_pred_lr)) #imprime los resultados de la regresion logistica\n",
    "\n",
    "# aqui se haria la comparacion grafica entre KNN y la regresion logistica la cual sera guerdada en la carpeta de figuras con el nombre de comparacion_knn_lr\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "plt.bar(['KNN', 'Logistic Regression'], [acc_knn, acc_lr], color=['#66c2a5', '#fc8d62'])\n",
    "plt.title('Comparación de Precisión')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('practica_ia/figuras/comparacion_knn_lr.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# parte 3 — clustering y PCA con dataset de peces\n",
    "\n",
    "# cargar el dataset de los peces el cual esta en la carpeta de datos\n",
    "fish = pd.read_csv('practica_ia/datos/fish.csv')\n",
    "fish = fish.dropna()\n",
    "\n",
    "# aqui selecionamos las variables que sean de tipo numero\n",
    "num_cols = fish.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# aqui el escalado\n",
    "data_scaled = StandardScaler().fit_transform(fish[num_cols])\n",
    "\n",
    "# aqui iniciamos el algoritmo PCA\n",
    "pca = PCA(n_components=2)\n",
    "fish_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# aqui asignamos el random state del KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(fish_pca)\n",
    "\n",
    "# aqui visualizamos el grafico que el PCA nos dejo el cual sera guardado en la carpeta figuras con el nombre de clustering_fish\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(fish_pca[:, 0], fish_pca[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title('Clustering de Peces (PCA)')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.savefig('practica_ia/figuras/clustering_fish.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# parte 4 — reglas de asociación con dataset transaccional sintético\n",
    "\n",
    "# generamos dataset transaccional sintético\n",
    "#asignamos el ramdon state o en este caso la random seed a 42 como todas las demas\n",
    "np.random.seed(42)\n",
    "items = ['Leche', 'Pan', 'Huevos', 'Queso', 'Mantequilla', 'Jugo', 'Arroz', 'Cereal', 'Pollo', 'Carne', 'Pasta', 'Yogurt']\n",
    "# creamos varios articulos o items con los nombres que estan arriba\n",
    "tickets = []\n",
    "for i in range(200):\n",
    "    n = np.random.randint(2, 7)\n",
    "    compra = np.random.choice(items, n, replace=False)\n",
    "    tickets.append(compra.tolist())\n",
    "\n",
    "# aqui convertimos la informacion en el formato one hot el cual \n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "oht = te.fit_transform(tickets)\n",
    "df_trans = pd.DataFrame(oht, columns=te.columns_)\n",
    "\n",
    "# aqui se aplican reglas de asociacion, por ejemplo si un cliente compra algo la mayoria de los casos comprara algo que atrae otro articulo por medio de este \n",
    "# osea ese articulo se asocia a otro\n",
    "frequent = apriori(df_trans, min_support=0.05, use_colnames=True)\n",
    "rules = association_rules(frequent, metric='confidence', min_threshold=0.4)\n",
    "\n",
    "# aqui guarda y muestra las reglas principales de la asociacion y los datos guardado se guardaran en la carpeta datos y se llamara reglas_asociacion \n",
    "print(\"\\n--- Principales Reglas de Asociación ---\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "rules.to_csv('practica_ia/datos/reglas_asociacion.csv', index=False)\n",
    "\n",
    "# conclusiones generales\n",
    "\n",
    "# KMeans permite detectar patrones en datos sin etiquetas.\n",
    "# Las reglas de asociación muestran relaciones prácticas entre productos, por ejemplo, huevos y cereal suelen comprarse juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34342c0d-ef65-4f33-b649-cb8ec6a1ca63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
